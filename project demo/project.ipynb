{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e487f7d-a292-4fcb-a762-2b933ef356c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using `keras.utils.Sequence` as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m valid_test_node_ids \u001b[38;5;241m=\u001b[39m [node_id \u001b[38;5;28;01mfor\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m test_node_ids \u001b[38;5;28;01mif\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m graph_train\u001b[38;5;241m.\u001b[39mnodes()]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train the model using valid node IDs\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_train_node_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use valid node IDs for training\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_test_node_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use valid node IDs for testing\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     88\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(generator\u001b[38;5;241m.\u001b[39mflow(data\u001b[38;5;241m.\u001b[39mloc[y_test\u001b[38;5;241m.\u001b[39mindex]), y_test)\n",
      "File \u001b[1;32mc:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1011\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1000\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1009\u001b[0m ):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(y):\n\u001b[1;32m-> 1011\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`y` argument is not supported when using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.utils.Sequence` as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1014\u001b[0m         )\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_none_or_empty(sample_weights):\n\u001b[0;32m   1016\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_weight` argument is not supported when using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.utils.Sequence` as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: `y` argument is not supported when using `keras.utils.Sequence` as input."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Path to the folders containing the data\n",
    "dyslexia_folder = \"Data/Dyslexic\"\n",
    "control_folder = \"Data/Control\"\n",
    "\n",
    "# Function to load data from folder\n",
    "def load_data_from_folder(folder, label):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            data = pd.read_csv(os.path.join(folder, filename))\n",
    "            # Create a unique identifier for each patient based on CSV file name\n",
    "            data[\"patient_id\"] = filename.split(\".\")[0]  # Extract patient ID from filename\n",
    "            data[\"label\"] = label  # Add label column\n",
    "            data_list.append(data)\n",
    "    return pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Load dyslexia samples\n",
    "D_data = load_data_from_folder(dyslexia_folder, \"dyslexia\")\n",
    "\n",
    "# Load control samples\n",
    "C_data = load_data_from_folder(control_folder, \"control\")\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "data = pd.concat([D_data, C_data], ignore_index=True)\n",
    "\n",
    "# Extract features (assuming 'LX', 'LY', 'RX', 'RY' are gaze coordinates)\n",
    "X = data[['LX', 'LY', 'RX', 'RY']].values\n",
    "\n",
    "# Scale the features (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Extract labels\n",
    "y = (data['label'] == 'dyslexia').astype(int)  # Binary labels (0 for control, 1 for dyslexia)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a StellarGraph object from the training data with node features\n",
    "# Ensure that the index used for nodes matches the size of X_train\n",
    "graph_train = sg.StellarGraph(nodes=pd.DataFrame(X_train, index=data.loc[y_train.index].index), edges=None)\n",
    "\n",
    "# Define the GraphSAGE node generator\n",
    "generator = GraphSAGENodeGenerator(graph_train, batch_size=50, num_samples=[5, 5])\n",
    "\n",
    "# Define the GraphSAGE model architecture\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32], generator=generator, bias=True, dropout=0.5\n",
    ")\n",
    "\n",
    "# Build the Keras model\n",
    "x_inp, x_out = graphsage_model.in_out_tensors()\n",
    "prediction = layers.Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "model = models.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_node_ids = data.loc[y_train.index].index.tolist()  # Get node IDs for training\n",
    "test_node_ids = data.loc[y_test.index].index.tolist()  # Get node IDs for testing\n",
    "\n",
    "# Filter out node IDs that do not exist in the graph\n",
    "valid_train_node_ids = [node_id for node_id in train_node_ids if node_id in graph_train.nodes()]\n",
    "valid_test_node_ids = [node_id for node_id in test_node_ids if node_id in graph_train.nodes()]\n",
    "\n",
    "# Train the model using valid node IDs\n",
    "history = model.fit(generator.flow(valid_train_node_ids),  # Use valid node IDs for training\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(generator.flow(valid_test_node_ids), y_test))  # Use valid node IDs for testing\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(generator.flow(data.loc[y_test.index]), y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8070add1-fadf-479b-80b6-9dd621f6a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m test_gen \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mflow(valid_test_node_ids, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model using the generators\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejanvjy4b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\sumantha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Get the set of valid node IDs that exist in the graph\n",
    "valid_graph_node_ids = set(graph_train.nodes())\n",
    "\n",
    "# Filter valid train and test node IDs\n",
    "valid_train_node_ids = [node_id for node_id in train_node_ids if node_id in valid_graph_node_ids]\n",
    "valid_test_node_ids = [node_id for node_id in test_node_ids if node_id in valid_graph_node_ids]\n",
    "\n",
    "# Prepare targets separately\n",
    "# For example, if y_train and y_test are already extracted\n",
    "# y_train = data.loc[y_train.index]['target_column'].values\n",
    "# y_test = data.loc[y_test.index]['target_column'].values\n",
    "\n",
    "# Now create the generators with valid node IDs\n",
    "train_gen = generator.flow(valid_train_node_ids, shuffle=True)\n",
    "test_gen = generator.flow(valid_test_node_ids, shuffle=False)\n",
    "\n",
    "# Train the model using the generators\n",
    "history = model.fit(train_gen, epochs=10, validation_data=(test_gen, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15cab4b1-3528-4ec1-a095-e5cfac7d98c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define input layer with appropriate shape\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[43mnum_features\u001b[49m,))  \u001b[38;5;66;03m# num_features should match the number of features in your data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Define model architecture\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dense_layer \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(input_layer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Example model with appropriate input layer\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Define input layer with appropriate shape\n",
    "input_layer = Input(shape=(num_features,))  # num_features should match the number of features in your data\n",
    "\n",
    "# Define model architecture\n",
    "dense_layer = Dense(units=64, activation='relu')(input_layer)\n",
    "output_layer = Dense(units=num_classes, activation='softmax')(dense_layer)  # num_classes is the number of output classes\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assuming train_gen and test_gen are correctly configured generators\n",
    "history = model.fit(train_gen, epochs=10, validation_data=test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "676ce7ea-428b-452b-bf1b-20883c3fb3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyg_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming pyg_data_list is a list of PyTorch Geometric Data objects\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ensure that pyg_data_list is defined and populated by calling graphs_to_pyg_data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mpyg_data_list\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Call train_model function with the DataLoader\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_model(model, dataloader, criterion, optimizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyg_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Assuming pyg_data_list is a list of PyTorch Geometric Data objects\n",
    "# Ensure that pyg_data_list is defined and populated by calling graphs_to_pyg_data\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(pyg_data_list, batch_size=1, shuffle=True)\n",
    "\n",
    "# Call train_model function with the DataLoader\n",
    "train_model(model, dataloader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8880a-f85d-4da1-b678-092c15f0c722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
