{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0f1842-69fe-41b4-9c22-8dc184437f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data_directory = 'Data/Control'\n",
    "dyslexia_data_directory = 'Data/Dyslexic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c945fcae-ceb8-4fa1-9588-278710ca2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Data preprocessing\n",
    "def read_csv_file(file_path, max_length=None):\n",
    "    # Read CSV file into a numpy array\n",
    "    data = np.loadtxt(file_path, delimiter=',', skiprows=1, usecols=(1, 2, 3, 4))\n",
    "    \n",
    "    # Pad or truncate sequences to max_length\n",
    "    if max_length:\n",
    "        if len(data) < max_length:\n",
    "            # Pad sequence with zeros if it's shorter than max_length\n",
    "            pad_width = ((0, max_length - len(data)), (0, 0))\n",
    "            data = np.pad(data, pad_width, mode='constant', constant_values=0)\n",
    "        elif len(data) > max_length:\n",
    "            # Truncate sequence if it's longer than max_length\n",
    "            data = data[:max_length]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_eye_tracking_data(directory, max_length=None, num_samples=None):\n",
    "    all_data = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            data = read_csv_file(file_path, max_length=max_length)\n",
    "            all_data.append(data)\n",
    "            count += 1\n",
    "            if num_samples and count >= num_samples:\n",
    "                break\n",
    "    return np.array(all_data)\n",
    "\n",
    "# Example usage with max_length and num_samples parameter\n",
    "control_eye_tracking_data = read_eye_tracking_data(control_data_directory, max_length=1000, num_samples=90)\n",
    "dyslexia_eye_tracking_data = read_eye_tracking_data(dyslexia_data_directory, max_length=1000, num_samples=90)\n",
    "\n",
    "# Assuming you have corresponding labels for the control and dyslexia groups\n",
    "# Combine the data and labels\n",
    "X = np.concatenate((control_eye_tracking_data, dyslexia_eye_tracking_data), axis=0)\n",
    "y = np.concatenate((np.zeros(len(control_eye_tracking_data)), np.ones(len(dyslexia_eye_tracking_data))), axis=0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e936a90f-78f7-415f-9c83-d2f81e2758f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUMANTHA\\AppData\\Local\\Temp\\ipykernel_15768\\579245363.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  X_train = (X_train - mean) / std\n",
      "C:\\Users\\SUMANTHA\\AppData\\Local\\Temp\\ipykernel_15768\\579245363.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  X_test = (X_test - mean) / std\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize the data if necessary\n",
    "# Example normalization\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ebca909-e29a-4153-8790-afd1736b1ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SUMANTHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SUMANTHA\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 5000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               640128    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 648449 (2.47 MB)\n",
      "Trainable params: 648449 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (1000, 5)  # Assuming your input data has 1000 samples and 5 features\n",
    "\n",
    "# Define your model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd20b0be-7935-4637-b089-36dd4afddfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 5000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               640128    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 648449 (2.47 MB)\n",
      "Trainable params: 648449 (2.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Shape of X_train: (144, 1000, 4)\n",
      "Shape of y_train: (144,)\n",
      "Example of preprocessed data: [[        nan         nan         nan         nan]\n",
      " [        nan  0.08354561  0.87727209 -0.06957523]\n",
      " [        nan -0.0387014   0.02901024 -0.10032509]\n",
      " ...\n",
      " [        nan -0.43258237 -1.68744862 -0.3956963 ]\n",
      " [        nan -0.41867296 -1.75027744 -0.43346822]\n",
      " [ 0.22604179 -0.36915089 -1.90001394 -0.3727809 ]]\n"
     ]
    }
   ],
   "source": [
    "# Check Model Architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Check Data Preprocessing\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "# Optionally, print some samples of preprocessed data to inspect their format\n",
    "print(\"Example of preprocessed data:\", X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78b5ec69-e5ea-41e2-82d9-8e146206bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1e98937-68c6-40fd-89c3-1fc51d5f015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_dataset(df):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    G.add_nodes_from(df['LX'].astype(str))\n",
    "\n",
    "    # Add edges\n",
    "    edges = [(str(row['LX']), str(row['LY'])) for index, row in df.iterrows()]\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70e2c3ce-d980-43bf-8ed2-97943083c16e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LX'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m dyslexia_eye_tracking_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dyslexia_eye_tracking_flat)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Use the tail() method on DataFrames\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m graph1 \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_graph_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_eye_tracking_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m graph2 \u001b[38;5;241m=\u001b[39m create_graph_from_dataset(dyslexia_eye_tracking_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[79], line 5\u001b[0m, in \u001b[0;36mcreate_graph_from_dataset\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add nodes\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Add edges\u001b[39;00m\n\u001b[0;32m      8\u001b[0m edges \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLX\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLY\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:395\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_loc(key, method\u001b[38;5;241m=\u001b[39mmethod, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'LX'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reshape the NumPy arrays to 2D\n",
    "control_eye_tracking_flat = control_eye_tracking_data.reshape(-1, control_eye_tracking_data.shape[-1])\n",
    "dyslexia_eye_tracking_flat = dyslexia_eye_tracking_data.reshape(-1, dyslexia_eye_tracking_data.shape[-1])\n",
    "\n",
    "# Convert NumPy arrays to Pandas DataFrames\n",
    "control_eye_tracking_df = pd.DataFrame(control_eye_tracking_flat)\n",
    "dyslexia_eye_tracking_df = pd.DataFrame(dyslexia_eye_tracking_flat)\n",
    "\n",
    "# Use the tail() method on DataFrames\n",
    "graph1 = create_graph_from_dataset(control_eye_tracking_df.tail())\n",
    "graph2 = create_graph_from_dataset(dyslexia_eye_tracking_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df91aa-4c62-4bad-8915-355d4faedbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a7608-e85d-4107-b4d9-8ace82a689e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1afcb-c47e-49e4-8ab0-07073563bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81662072-1c8d-41a7-9903-e2dd753cf4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f59bced-834a-48e5-bcc7-cc4c62a0b869",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tail'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m graph1 \u001b[38;5;241m=\u001b[39m create_graph_from_dataset(\u001b[43mcontrol_eye_tracking_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m())\n\u001b[0;32m      2\u001b[0m graph2 \u001b[38;5;241m=\u001b[39m create_graph_from_dataset(dyslexia_eye_tracking_data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the graphs\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tail'"
     ]
    }
   ],
   "source": [
    "graph1 = create_graph_from_dataset(control_eye_tracking_data.tail())\n",
    "graph2 = create_graph_from_dataset(dyslexia_eye_tracking_data.head())\n",
    "\n",
    "# Plot the graphs\n",
    "plt.figure(figsize=(50, 30))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pos1 = nx.spring_layout(graph1)\n",
    "nx.draw(graph1, pos1, with_labels=True, font_size=8, node_color='skyblue', node_size=300, font_color='black', edge_color='gray', linewidths=0.5)\n",
    "plt.title('Graph from Dataset 1')\n",
    "\n",
    "#plt.subplot(1, 2, 2)\n",
    "#pos2 = nx.spring_layout(graph2)\n",
    "#nx.draw(graph2, pos2, with_labels=True, font_size=8, node_color='lightcoral', node_size=300, font_color='black', edge_color='gray', linewidths=0.5)\n",
    "#plt.title('Graph from Dataset 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a19c7b-fbf8-4462-9f72-dcfb4ceb920b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9468c-c402-485f-a4d1-bd16a76e7563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
